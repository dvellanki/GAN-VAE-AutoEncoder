# Variational Auto Encoders (VAE)

VAE are latent variable models. Such models rely on the idea that the data generated by the model needs to be parametrised by some variables that will generate some specific caracteristics of that given data point. These varaibles are called latent variables. 

One of the key idea behind VAE is that instead of trying to construct a latent space (space of latent variables) explicitly and to sample from it in order to find samples that could actually generate proper outputs (as close as possible to our distribution), we construct an Encoder-Decoder like netwkork which will split the work in two parts:

- The encoder learns to generate a distribution depending on input samples X from which we can sample a latent variable that is highly likelly to generate X samples. In other words we learn a set of parameters θ1 that generate a distribution Q(X,θ1)  from which we can sample a latent variable z maximizing P(X|z).
- The decoder part learns to generate an output which belongs to the real data distribution given a latent variable z as an input. In other words, we learn a set of parameters θ2 that generates a function f(z,θ2) that maps the latent distibution that we learned to the real data distribution of the dataset.

![](imgs/vae-gaussian.png)



In order to understand the matematics behind Variational Auto Encoders, we will go throught the theory and see why these models works better than older approaches.

### This article will cover the following

- How to define the construct the latent space
- How to generate data efficiently from latent space sampling.
- The final architecture of VAEs
- Some experiments showing intresting properties of VAEs



## 1. Latent Variable Models

Latent variable models come from the idea that the data generated by a model needs to be parametrised by *latent* variables. This name comes from the fact that given just a data point produced by the model, we don’t necessarily know which settings of the latent variables generated this data point.

In a more formal setting, we have a vector of latent variables *z* in a high-dimensional space Z which we can easily sample according to some probability density function *P*(*z*) defined over Z. Then, we have a family of deterministic functions *f* (*z*; *θ*), parameterized by a vector *θ* in some space Θ, where *f* :Z×Θ→X. *f* is deterministic, but if *z* is random and *θ* is fixed, then *f* (*z*; *θ*) is a random variable in the space X . 

During training, we optimize *θ* such that we can sample *z* from *P*(*z*) and, with high probability, having *f* (*z*; *θ*) as close as the *X*’s in the dataset.

In order to achieve that, we need to find the parameters *θ* such that:
$$
\int \underbrace{P(X|z;θ)}_{f(z;θ)} P(z)dz = P(X)
$$
Here, we just replace *f* (*z*; *θ*)  by a distribution *P*(*X*|*z*; *θ*) in order to make the dependence of *X* on *z* explicit by using the law of total probability. An other assumption that we make is to suppose that P(W|z;*θ*) follow a Gaussian distribution N(X|*f* (*z*; *θ*), σ*I) (By doing so we consider that generated data are almost as X but not exactly X).

#### Defining the latent space

As explained in the begening, the latent space is supposed to model a space of variables influencing some specific carateristics of our data distribution. We can imagine that if the dataset that we consider is composed of cars and that our data distribution is then the space of all possible cars, some components of our latent vector would influence the color, the orientation or the number of doors of a car. 

However, it is rapidly very tricky to explicitly define the role of each latent components, particularly when we are dealing with hundreds of dimensions. In addition to that some component can depends on others which makes it even more complex to design by hand this latent space. In other words, it's really difficult to define this complex distribution P(z). 

#### Solution

In order to overcome this issue, the trick is to use a mathematical property of probability distributions and the ability of neural networks to learn some deterministic functions under some constrains with backpropagation.

The mathematical property that makes the problem way more tractable is that *any distribution in d dimensions can be generated by taking a set of d variables that are normally distributed and mapping them through a sufficiently complicated function.* As a consequence, we can arbitrarily decide that our latent variables to be Gaussians and then construct a deterministic function that will map our Gaussian latent space into the complex distribution from which we will sample to generate our data. 

The deterministic function needed to map our simple latent distribution into a more complex one that would represent our complex latent space can then be build using a neural network with some parameters that can be fine tuned during training.



## 2. Learn to generate data from the latent space

Before juming into the interesting part of this article, let's recall our final goal:

We have a d dimensional latent space which is normally distributed and we want to learn a function f(z;θ2) that will map our latent distribution to our real data distribution. In other words we want to sample latent variables and then use this latent variable as an input of our generator in order to generate a data sample that will be as close as possible of a real data points.

**We still need to resolve two things:**

- How do we explore our latent space efficiently in order to discover the z that will maximize the probability P(X|z) ? (we need to find the right z for a given X during training)
- How do we train this all process using back propagation ? (we need to find an objective that will optimize f to map P(z) to P(X))



### Finding the right z latent variable for our X data sample

In practice, for most *z*, *P*(X|z) will be nearly zero, and hence contribute almost nothing to our estimate of *P*(X). The key idea behind the variational autoencoder is to attempt to sample values of z that are likely to have produced X, and compute P(X) just from those. In order to do that, we need a new function *Q*(z|X) which can take a value of X and give us a distribution over z values that are likely to produce X. Hopefully the space of z values that are likely under Q will be much smaller than the space of all z’s that are likely under the prior P(z).

This part of the VAE will be the encoder and we will assume that Q will be learned during training by a neural network mapping the input X to the output Q(z|X) which will be the distribution from which we are most likely to find a good z to generate this particular X.

### Training the model with backpropagation

In order to understand how to train our VAE, we first need to define what should be the objective, and to do so, we will first need to do a little bit of maths.

Let's start with the Encoder, we want Q(z|X) to be as close as possible to P(X|z). In order to measure how close the two distributions are, we can use the Kullback-Leibler divergence D between the two distributions:


$$
D [Q(z|X)∥P(z|X)] = E_{z∼Q} [\log Q(z|X) − |log P(z|X)] .
$$
With a little bit of maths, we can rewrite this equality in a more intersting way.

By applying the Bayes rule on P(z|X) we have:
$$
D[Q(z|X)∥P(z|X)] = E_{z∼Q} [\log Q(z|X) − \log P(X|z) − \log P(z)] + \log P(X)  \\
$$
Which is equivalent to:
$$
\underbrace{\log P(X) − D [Q(z|X)∥P(z|X)]}_{Part A} = \underbrace{E_{z∼Q} [\log P(X|z)] − D [Q(z|X)∥P(z)]}_{Part B}
$$


**Let's take a time to look at this formulae**

- **Part A**: The left term is not really interesting for our backpropagation setting (we don't know a simple expression for P(X)), but log(P(X)) is actually what we want to maximize given a z and we can see here that we can do so by minimizing the right part (making Q(z|X) as close as possible to P(z|X)). This is exactly what we mentionned at the beginning.
- **Part B**: This term is much more interesting as we know P(X|z) (it's our decoder part -> generator) and Q(z|X) (it's our encoder). We can see here that in order to maximize this term, we need to maximize log(P(X|z)) which means that we wan't to maximize the log likelihood of our probability and minimize the KL Divergence between Q(z|X) and P(z).

In order to make **Part B** more easy to compute is to suppose that Q(z|X) is a gaussian distribution N(z|mu(X,θ1), sigma(X,θ1)) where θ1 are the parameters learned by our neural network from our data set.

One issue remains unclear with our formulae : How do we compute the expectation during backpropagation ?

**Handling the expectation operator**

One way would be to do multiple forward pass in order to be able to compute the expectation of the log(P(X|z)) but this is computationally inefficient. Hopefully, as we are in a stochastic training, we can supposed that the data sample Xi that we we use during the epoch is representative of the entire dataset and thus it is reasonable to consider that the log(P(Xi|zi)) that we obtain from this sample Xi and the dependently generated zi is representative of the expectation over Q of log(P(X|z)).



## 3. Final architecture of VAEs



We can know resume the final architecture of a VAE. As announced in the introduction, the netwkork is splitted in two parts:

- The encoder that learns to generate a distribution depending on input samples X from which we can sample a latent variable that is highly likelly to generate X samples. This part needs to be optimized in order to enforce our Q(z|X) to be gaussian.

- The decoder part learns to generate an output which belongs to the real data distribution given a latent variable z as an input. This part maps a sampled z (initially from a normal distribution) into a more complex latent space (the one actually representing our data) and from this complex latent variable z generate a data point wich is as close as possible to a real data point from our distribution.

    

![](imgs/vae-detailed-architecture.png)



## 4. Experiments with VAEs



### Implementation of a VAE Network

#### Encoder

```python
encoder = nn.Sequential(nn.Linear(28 * 28, 256),
                        nn.ReLU(),
                        nn.Linear(256, 128))
```



#### Decoder

```python
decoder = nn.Sequential(nn.Linear(128, 256),
                        nn.ReLU(),
                        nn.Linear(256, 28 * 28))
```



#### Global architecture

```python
class VAE(nn.Module):
    def __init__(self, latent_dim):
        super().__init__()
        
        self.encoder = nn.Sequential(nn.Linear(28 * 28, 256),
                                     nn.ReLU(),
                                     nn.Linear(256, 128))
        
        self.mu     = nn.Linear(128, latent_dim)
        self.logvar = nn.Linear(128, latent_dim)
        
        self.latent_mapping = nn.Linear(latent_dim, 128)
        
        self.decoder = nn.Sequential(nn.Linear(128, 256),
                                     nn.ReLU(),
                                     nn.Linear(256, 28 * 28))
        
        
    def encode(self, x):
        x = x.view(x.size(0), -1)
        encoder = self.encoder(x)
        mu, logvar = self.mu(encoder), self.logvar(encoder)
        return mu, logvar
        
    def sample_z(self, mu, logvar):
        eps = torch.rand_like(mu)
        return mu + eps * torch.exp(0.5 * logvar)
    
    def decode(self, z,x):
        latent_z = self.latent_mapping(z)
        out = self.decoder(latent_z)
        reshaped_out = torch.sigmoid(out).view(x.shape[0],1, 28,28)
        return reshaped_out
        
    def forward(self, x):
        
        mu, logvar = self.encode(x)
        z = self.sample_z(mu, logvar)
        output = self.decode(z,x)
        
        return output
```





### Continuity in the latent space

![](imgs/vae-continuity.png)

## Conclusion



### References

### Code